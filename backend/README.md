# Spur AI Chat â€“ Backend

This is the backend service for the Spur AI Chat Support application.  
It handles message processing, session management, persistence, and AI-powered replies.

---

##  Tech Stack

- **Runtime:** Node.js
- **Framework:** Express
- **Database:** PostgreSQL
- **ORM:** Prisma
- **LLM Provider:** Groq (LLaMA model)

---

##  Features

- Session-based conversations
- Persistent storage of conversations and messages
- Clean controller â†’ service â†’ data layer separation
- LLM replies with conversation context
- Graceful error handling

---

##  Project Structure

src/
â”œâ”€ controllers/ # Request handling
â”œâ”€ services/ # LLM + business logic
â”œâ”€ routes/ # API routes
â”œâ”€ prisma/ # Prisma schema & migrations
â””â”€ server.js # App entry point

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Install dependencies

```bash
npm install

2ï¸âƒ£ Environment variables
Create a .env file in backend/:
env
Copy code
DATABASE_URL=postgresql://USER:PASSWORD@HOST:PORT/DB_NAME
GROQ_API_KEY=your_groq_api_key

3ï¸âƒ£ Database setup
npx prisma migrate dev
(Optional)
npx prisma studio

4ï¸âƒ£ Run the server

npm run dev

Server runs at:
http://localhost:4000

ğŸ”— API Endpoints
POST /chat/message
Request

json
Copy code
{
  "message": "What is your return policy?",
  "sessionId": "optional-session-id"
}
Response

json
Copy code
{
  "reply": "AI-generated response",
  "sessionId": "conversation-session-id"
}

Data & Persistence
Conversations are identified using a session ID

All user and assistant messages are stored in the database

Conversation history is used to maintain multi-turn context

Sessions persist across page reloads via localStorage

Notes & Trade-offs

Authentication is intentionally omitted (not required by assignment)

Responses are generated synchronously (no streaming)

Designed for clarity and extensibility over feature completeness

ğŸ‘¤ Author
Archit Prakash Choudhary
